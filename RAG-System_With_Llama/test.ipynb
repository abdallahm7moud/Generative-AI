{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3e325c",
   "metadata": {},
   "source": [
    "### Verify Ollama is accessible via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11bdd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can be used in a variety of ways, from helping you plan a vacation to creating art. I'm here to assist you in finding the help or information you need. My strengths include answering questions, generating text and images and even just chatting with you.\n"
     ]
    }
   ],
   "source": [
    "# Create a new Python file named test_ollama.py\n",
    "import requests\n",
    "# Test the Ollama API\n",
    "response = requests.post(\n",
    "\"http://localhost:11434/api/generate\",\n",
    "json={\n",
    "    \"model\": \"llama3.2:1b\",\n",
    "    \"prompt\": \"Hello, what are your capabilities?\",\n",
    "    \"stream\": False\n",
    "}\n",
    ")\n",
    "print(response.json()[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3436e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ITI\\Generative AI and prompt Engineering\\RAG-System\\rag\\embeddings.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "d:\\ITI\\Generative AI and prompt Engineering\\RAG-System\\rag-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\ITI\\Generative AI and prompt Engineering\\RAG-System\\rag\\embeddings.py:60: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store from vectorstore\n",
      "Connected to Ollama with model: llama3.2:1b\n",
      "Ollama RAG system initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ITI\\Generative AI and prompt Engineering\\RAG-System\\rag\\generator.py:24: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.llm = Ollama(model=model_name, temperature=temperature)\n",
      "d:\\ITI\\Generative AI and prompt Engineering\\RAG-System\\rag\\retriever.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  documents = self.retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           query  \\\n",
      "0        What is RAG and when was it introduced?   \n",
      "1  What are the main components of a RAG system?   \n",
      "2          What are the advantages of using RAG?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     response  \\\n",
      "0                                                                                                                                                                                                Based on the provided context, I can infer that Retrieval-Augmented Generation (RAG) is a technique that enhances large language models by combining retrieval-based and generation-based methods.\\n\\nRAG was introduced in [Document 1], which addresses the limitations of traditional language models such as outdated knowledge and hallucinations. This suggests that RAG has been developed to address specific challenges in natural language processing.\\n\\nUnfortunately, I don't have more information on when RAG was specifically introduced, but it is likely a relatively recent development in the field of NLP [Document 1].   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                            Based on [Document 1], the main components of a Retrieval-Augmented Generation (RAG) system include:\\n\\n1. A document store containing knowledge\\n2. A retrieval system to find relevant information\\n3. A language model to generate responses\\n\\nThese components work together to enhance large language models by allowing them to access external knowledge and generating accurate responses [Document 1].   \n",
      "2  Based on the provided context, the advantages of using Retrieval-Augmented Generation (RAG) include:\\n\\n1. **Improved accuracy**: By grounding responses in factual information from external sources, RAG can reduce the likelihood of outdated knowledge and hallucinations [Document 1].\\n2. **Enhanced contextual understanding**: The retrieval system allows for more accurate and relevant information to be retrieved, leading to better contextual understanding of the input [Document 2].\\n3. **Increased flexibility**: RAG systems can adapt to new information sources and update their knowledge base over time, making them more flexible than traditional language models [Document 1].\\n\\nThese advantages make RAG a promising approach for improving the performance of large language models in various applications.   \n",
      "\n",
      "   precision  recall  f1_score  context_utilization  has_citations  \n",
      "0        0.0     0.0         0             0.521739           True  \n",
      "1        0.0     0.0         0             0.800000           True  \n",
      "2        0.0     0.0         0             0.463415           True  \n"
     ]
    }
   ],
   "source": [
    "# examples/test_evaluator.py\n",
    "from main import OllamaRAGSystem\n",
    "from rag.evaluator import SimpleRAGEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the RAG system\n",
    "rag = OllamaRAGSystem(\n",
    "    data_dir=\"data\",\n",
    "    ollama_model=\"llama3.2:1b\",\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "# Create test cases\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"What is RAG and when was it introduced?\",\n",
    "        \"relevant_docs\": [\"data/rag_explanation.txt\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the main components of a RAG system?\",\n",
    "        \"relevant_docs\": [\"data/rag_explanation.txt\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the advantages of using RAG?\",\n",
    "        \"relevant_docs\": [\"data/rag_explanation.txt\"]\n",
    "    }\n",
    "]\n",
    " \n",
    "# Initialize evaluator\n",
    "evaluator = SimpleRAGEvaluator()\n",
    "# Run evaluation\n",
    "results = evaluator.run_evaluation(rag, test_queries)\n",
    "# Display results\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(results[[\"query\", \"response\", \"precision\", \"recall\", \"f1_score\", \"context_utilization\", \"has_citations\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
